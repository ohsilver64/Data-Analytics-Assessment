{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfcde69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "--- SECTION 1: LIBRARY IMPORTS ---\n",
    "Importing necessary libraries for deep learning (PyTorch), machine learning (Scikit-learn, XGBoost, CatBoost),\n",
    "data manipulation (Pandas, NumPy), and system operations.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"mps\") if torch.backends.mps.is_available() else (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "IMPORTANT_FEATURES = [\n",
    "    'MagpieData maximum MendeleevNumber', 'MagpieData mean AtomicWeight',\n",
    "    'MagpieData minimum MeltingT', 'MagpieData maximum MeltingT',\n",
    "    'MagpieData mean MeltingT', 'MagpieData minimum Column',\n",
    "    'MagpieData range Column', 'MagpieData avg_dev Column',\n",
    "    'MagpieData mode Column', 'MagpieData range Row', 'MagpieData mean Row',\n",
    "    'MagpieData range Electronegativity', 'MagpieData avg_dev Electronegativity',\n",
    "    'MagpieData mode Electronegativity', 'MagpieData mean NpValence',\n",
    "    'MagpieData maximum NdValence', 'MagpieData range NdValence',\n",
    "    'MagpieData mean NdValence', 'MagpieData maximum NfValence',\n",
    "    'MagpieData mean NfValence', 'MagpieData mean NValence',\n",
    "    'MagpieData mode NValence', 'MagpieData maximum NpUnfilled',\n",
    "    'MagpieData range NpUnfilled', 'MagpieData mean NpUnfilled',\n",
    "    'MagpieData range NUnfilled', 'MagpieData mean NUnfilled',\n",
    "    'MagpieData mode NUnfilled', 'MagpieData minimum GSvolume_pa',\n",
    "    'MagpieData mode GSvolume_pa', 'MagpieData maximum GSbandgap',\n",
    "    'MagpieData range GSbandgap', 'MagpieData mode GSbandgap',\n",
    "    'MagpieData mean GSmagmom', 'MagpieData mode SpaceGroupNumber'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0af0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- SECTION 2: PREDICTION MODEL DEFINITIONS (REGRESSION) ---\n",
    "Definitions for Deep Learning regression models (CNN, ViT, Diffusion) and a factory function\n",
    "to instantiate traditional machine learning regressors.\n",
    "\"\"\"\n",
    "class Simple2DCNN(nn.Module):\n",
    "    def __init__(self, input_side_len):\n",
    "        super(Simple2DCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if input_side_len == 12: flatten_dim = 128 * 3 * 3\n",
    "        elif input_side_len == 6: flatten_dim = 128 * 1 * 1\n",
    "        else: raise ValueError(\"Unsupported image size\")\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flatten_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.regressor(self.features(x))\n",
    "\n",
    "class ImageViT(nn.Module):\n",
    "    def __init__(self, image_size, dim=64, depth=4, heads=4, mlp_dim=128):\n",
    "        super().__init__()\n",
    "        num_patches = image_size * image_size \n",
    "        self.pixel_embedding = nn.Linear(1, dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, 1))\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.view(b, h*w, 1)\n",
    "        x = self.pixel_embedding(x)\n",
    "        cls_tokens = self.cls_token.repeat(b, 1, 1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(h*w + 1)]\n",
    "        x = self.transformer(x)\n",
    "        return self.mlp_head(x[:, 0])\n",
    "\n",
    "class DiffusionNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(nn.Linear(1, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.cond_mlp = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.input_mlp = nn.Linear(1, hidden_dim)\n",
    "        self.mid = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, 1))\n",
    "    def forward(self, x, t, condition):\n",
    "        t_emb = self.time_mlp(t.view(-1, 1) / 100.0)\n",
    "        c_emb = self.cond_mlp(condition)\n",
    "        x_emb = self.input_mlp(x)\n",
    "        return self.mid(x_emb + t_emb + c_emb)\n",
    "\n",
    "class DiffusionWrapper(nn.Module):\n",
    "    def __init__(self, input_dim, n_steps=100):\n",
    "        super().__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.net = DiffusionNet(input_dim)\n",
    "        self.register_buffer('betas', torch.linspace(1e-4, 0.02, n_steps))\n",
    "        self.register_buffer('alphas', 1 - self.betas)\n",
    "        self.register_buffer('alphas_cumprod', torch.cumprod(self.alphas, dim=0))\n",
    "    def compute_loss(self, x0, condition):\n",
    "        batch_size = x0.shape[0]\n",
    "        t = torch.randint(0, self.n_steps, (batch_size,), device=x0.device)\n",
    "        noise = torch.randn_like(x0)\n",
    "        a_bar = self.alphas_cumprod[t].view(-1, 1)\n",
    "        x_t = torch.sqrt(a_bar) * x0 + torch.sqrt(1 - a_bar) * noise\n",
    "        pred_noise = self.net(x_t, t.float(), condition)\n",
    "        return F.mse_loss(pred_noise, noise)\n",
    "    @torch.no_grad()\n",
    "    def sample(self, condition, n_ensembles=20):\n",
    "        batch_size = condition.shape[0]\n",
    "        cond_expanded = condition.repeat_interleave(n_ensembles, dim=0)\n",
    "        x = torch.randn(cond_expanded.shape[0], 1, device=condition.device)\n",
    "        for t in reversed(range(self.n_steps)):\n",
    "            t_batch = torch.full((x.shape[0],), t, device=condition.device, dtype=torch.float)\n",
    "            pred_noise = self.net(x, t_batch, cond_expanded)\n",
    "            beta = self.betas[t]\n",
    "            alpha = self.alphas[t]\n",
    "            a_bar = self.alphas_cumprod[t]\n",
    "            if t > 0: noise = torch.randn_like(x)\n",
    "            else: noise = 0\n",
    "            mean = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - a_bar)) * pred_noise)\n",
    "            x = mean + torch.sqrt(beta) * noise\n",
    "        x = x.view(batch_size, n_ensembles, 1)\n",
    "        return x.mean(dim=1)\n",
    "\n",
    "def get_regressor(reg_type):\n",
    "    if reg_type == 'XGBoost': return XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, n_jobs=-1, random_state=42)\n",
    "    if reg_type == 'CatBoost': return CatBoostRegressor(iterations=1000, learning_rate=0.05, verbose=0, random_seed=42)\n",
    "    if reg_type == 'RandomForest': return RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=42)\n",
    "    if reg_type == 'KRR': return KernelRidge(kernel='rbf', alpha=0.1, gamma=0.01)\n",
    "    if reg_type == 'SVR': return SVR(kernel='rbf', C=10, gamma=0.01)\n",
    "    if reg_type == 'MLP': return MLPRegressor(hidden_layer_sizes=(256, 128, 64), max_iter=500, random_state=42)\n",
    "    if reg_type == 'Ridge': return Ridge(alpha=1.0)\n",
    "    if reg_type == 'Lasso': return Lasso(alpha=0.01)\n",
    "    if reg_type == 'DecisionTree': return DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "    raise ValueError(f\"Unknown regressor: {reg_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f3ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- SECTION 3: CLASSIFICATION MODEL DEFINITIONS ---\n",
    "Factory function to instantiate various traditional machine learning classifiers\n",
    "used for the two-step metal/non-metal classification task.\n",
    "\"\"\"\n",
    "def get_classifier(clf_type):\n",
    "    if clf_type == 'XGBoost': return XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=6, random_state=42, n_jobs=-1)\n",
    "    if clf_type == 'CatBoost': return CatBoostClassifier(iterations=500, verbose=0, random_seed=42)\n",
    "    if clf_type == 'RandomForest': return RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n",
    "    if clf_type == 'DecisionTree': return DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "    if clf_type == 'SVM': return SVC(kernel='rbf', probability=True, random_state=42)\n",
    "    if clf_type == 'LogisticRegression': return LogisticRegression(max_iter=1000, random_state=42)\n",
    "    if clf_type == 'LDA': return LinearDiscriminantAnalysis()\n",
    "    raise ValueError(f\"Unknown classifier: {clf_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df5ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- SECTION 4: DATASET PROCESSING DEFINITIONS ---\n",
    "Functions to reshape and pad feature matrices based on the model architecture requirements\n",
    "(e.g., transforming 1D vectors to 2D image formats for CNNs/ViTs).\n",
    "\"\"\"\n",
    "def process_features(X_in, model_type, use_important_features):\n",
    "    is_image_model = model_type in ['CNN', 'ViT']\n",
    "    \n",
    "    if is_image_model:\n",
    "        if use_important_features:\n",
    "            pad_width = ((0,0), (1,0)) \n",
    "            X_out = np.pad(X_in, pad_width, mode='constant').reshape(-1, 1, 6, 6)\n",
    "        else:\n",
    "            target = 144\n",
    "            if X_in.shape[1] > target: X_in = X_in[:, :target]\n",
    "            pad = target - X_in.shape[1]\n",
    "            X_out = np.pad(X_in, ((0,0), (0, pad)), mode='constant').reshape(-1, 1, 12, 12)\n",
    "    else:\n",
    "        X_out = X_in\n",
    "        \n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "454e945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- SECTION 5: TRAIN-TEST FUNCTION DEFINITION ---\n",
    "The main execution loop implementing a 5-Fold Cross-Validation strategy.\n",
    "Handles data splitting, optional two-step classification, feature scaling,\n",
    "model training (both DL and ML), and final evaluation.\n",
    "\"\"\"\n",
    "def train_test_cv(model_type='XGBoost', classifier_type=None, use_important_features=True, \n",
    "                  csv_path='team-a.csv', n_splits=5, epochs=50, batch_size=64):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"5-FOLD CV: Regressor={model_type} | Classifier={classifier_type} | Features={'Important' if use_important_features else 'All'}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"CSV not found, generating dummy data.\")\n",
    "        df = pd.DataFrame(np.random.rand(500, 100), columns=[f'feat_{i}' for i in range(100)])\n",
    "        for c in IMPORTANT_FEATURES: df[c] = np.random.rand(500)\n",
    "        df['gap expt'] = np.abs(np.random.randn(500))\n",
    "    else:\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "    y_raw = df.iloc[:, 1].values\n",
    "    \n",
    "    if use_important_features:\n",
    "        cols = [c for c in IMPORTANT_FEATURES if c in df.columns]\n",
    "        X_raw = df[cols].values\n",
    "    else:\n",
    "        X_raw = df.iloc[:, 2:].values\n",
    "\n",
    "    X_train_val, X_holdout_test, y_train_val, y_holdout_test = train_test_split(X_raw, y_raw, test_size=0.2, random_state=42)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_mae_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_val)):\n",
    "        print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "        \n",
    "        X_fold_train, X_fold_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "        scaler_x = StandardScaler()\n",
    "        X_fold_train = scaler_x.fit_transform(X_fold_train)\n",
    "        X_fold_val = scaler_x.transform(X_fold_val)\n",
    "        \n",
    "        if classifier_type is not None:\n",
    "            y_cls_train = (y_fold_train > 0.001).astype(int)\n",
    "            \n",
    "            clf = get_classifier(classifier_type)\n",
    "            clf.fit(X_fold_train.reshape(X_fold_train.shape[0], -1), y_cls_train)\n",
    "            \n",
    "            y_cls_pred_val = clf.predict(X_fold_val.reshape(X_fold_val.shape[0], -1))\n",
    "            \n",
    "            non_metal_mask = y_cls_train == 1\n",
    "            X_reg_train = X_fold_train[non_metal_mask]\n",
    "            y_reg_train = y_fold_train[non_metal_mask]\n",
    "            \n",
    "        else:\n",
    "            y_cls_pred_val = np.ones(len(y_fold_val))\n",
    "            X_reg_train = X_fold_train\n",
    "            y_reg_train = y_fold_train\n",
    "\n",
    "        scaler_y = QuantileTransformer(output_distribution='normal', n_quantiles=min(500, len(y_reg_train)//2))\n",
    "        y_reg_train_scaled = scaler_y.fit_transform(y_reg_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "        X_reg_train_processed = process_features(X_reg_train, model_type, use_important_features)\n",
    "        X_fold_val_processed = process_features(X_fold_val, model_type, use_important_features)\n",
    "        \n",
    "        input_shape = X_reg_train_processed.shape\n",
    "        \n",
    "        if model_type in ['CNN', 'ViT', 'Diffusion']:\n",
    "            train_ds = TensorDataset(torch.tensor(X_reg_train_processed, dtype=torch.float32), \n",
    "                                     torch.tensor(y_reg_train_scaled, dtype=torch.float32))\n",
    "            y_fold_val_scaled = scaler_y.transform(y_fold_val.reshape(-1, 1)).ravel()\n",
    "            val_ds = TensorDataset(torch.tensor(X_fold_val_processed, dtype=torch.float32), \n",
    "                                   torch.tensor(y_fold_val_scaled, dtype=torch.float32))\n",
    "            \n",
    "            train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            if model_type == 'CNN': model = Simple2DCNN(input_shape[2]).to(DEVICE)\n",
    "            elif model_type == 'ViT': model = ImageViT(input_shape[2]).to(DEVICE)\n",
    "            elif model_type == 'Diffusion': model = DiffusionWrapper(input_shape[1]).to(DEVICE)\n",
    "            \n",
    "            opt = optim.AdamW(model.parameters() if model_type!='Diffusion' else model.net.parameters(), lr=1e-3)\n",
    "            crit = nn.MSELoss()\n",
    "            \n",
    "            best_val_mse = float('inf')\n",
    "            best_w = None\n",
    "            patience = 0\n",
    "            \n",
    "            for ep in range(epochs):\n",
    "                model.train()\n",
    "                for bx, by in train_loader:\n",
    "                    bx, by = bx.to(DEVICE), by.to(DEVICE)\n",
    "                    opt.zero_grad()\n",
    "                    loss = model.compute_loss(by.unsqueeze(1), bx) if model_type=='Diffusion' else crit(model(bx), by.unsqueeze(1))\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                \n",
    "                model.eval()\n",
    "                val_mse = 0\n",
    "                with torch.no_grad():\n",
    "                    for bx, by in val_loader:\n",
    "                        bx, by = bx.to(DEVICE), by.to(DEVICE)\n",
    "                        loss = model.compute_loss(by.unsqueeze(1), bx) if model_type=='Diffusion' else crit(model(bx), by.unsqueeze(1))\n",
    "                        val_mse += loss.item()\n",
    "                \n",
    "                if val_mse < best_val_mse:\n",
    "                    best_val_mse = val_mse\n",
    "                    best_w = copy.deepcopy(model.net.state_dict() if model_type=='Diffusion' else model.state_dict())\n",
    "                    patience = 0\n",
    "                else: patience += 1\n",
    "                if patience >= 10: break\n",
    "            \n",
    "            if model_type=='Diffusion': model.net.load_state_dict(best_w)\n",
    "            else: model.load_state_dict(best_w)\n",
    "            model.eval()\n",
    "            \n",
    "            X_val_tensor = torch.tensor(X_fold_val_processed, dtype=torch.float32).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                if model_type=='Diffusion': pred_scaled = model.sample(X_val_tensor, n_ensembles=10).cpu().numpy()\n",
    "                else: pred_scaled = model(X_val_tensor).cpu().numpy()\n",
    "        \n",
    "        else:\n",
    "            reg = get_regressor(model_type)\n",
    "            if X_reg_train_processed.ndim > 2:\n",
    "                reg.fit(X_reg_train_processed.reshape(len(X_reg_train_processed), -1), y_reg_train_scaled)\n",
    "                pred_scaled = reg.predict(X_fold_val_processed.reshape(len(X_fold_val_processed), -1)).reshape(-1, 1)\n",
    "            else:\n",
    "                reg.fit(X_reg_train_processed, y_reg_train_scaled)\n",
    "                pred_scaled = reg.predict(X_fold_val_processed).reshape(-1, 1)\n",
    "\n",
    "        pred_raw = scaler_y.inverse_transform(pred_scaled).flatten()\n",
    "        \n",
    "        final_pred = pred_raw * y_cls_pred_val\n",
    "        final_pred = np.maximum(final_pred, 0.0)\n",
    "        \n",
    "        mae = mean_absolute_error(y_fold_val, final_pred)\n",
    "        fold_mae_scores.append(mae)\n",
    "        print(f\"  [Fold {fold+1}] MAE: {mae:.4f}\")\n",
    "\n",
    "    avg_mae = np.mean(fold_mae_scores)\n",
    "    std_mae = np.std(fold_mae_scores)\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(f\"CV Results (Aligns with Baseline):\")\n",
    "    print(f\"Mean MAE: {avg_mae:.4f} (+/- {std_mae * 2:.4f})\")\n",
    "    print(f\"{'='*30}\")\n",
    "    \n",
    "    return avg_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a77b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: Regressor=CNN | Classifier=None\n",
      "Features: All\n",
      "============================================================\n",
      "Using all 132 features.\n",
      "Step 1: Skipped (Pure Regression Mode)\n",
      "Step 2: Training CNN Regressor...\n",
      "Ep 10 | Val Loss: 3.7435\n",
      "Ep 20 | Val Loss: 3.6084\n",
      "Ep 30 | Val Loss: 3.4448\n",
      "\n",
      "Final Test Results:\n",
      "Regressor: CNN + Classifier: None\n",
      "MAE:  0.5776\n",
      "RMSE: 1.2735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5776020898835795"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(model_type='CNN', use_important_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "092b84e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: Regressor=CatBoost | Classifier=SVM\n",
      "Features: All\n",
      "============================================================\n",
      "Using all 132 features.\n",
      "Step 1: Training SVM Classifier for Metal/Non-Metal...\n",
      "Classifier Accuracy on Test Set: 0.8838\n",
      "Data Filtered: Regressor will train on 1530/3222 samples (Non-Metals).\n",
      "Step 2: Training CatBoost Regressor...\n",
      "\n",
      "Final Test Results:\n",
      "Regressor: CatBoost + Classifier: SVM\n",
      "MAE:  0.4484\n",
      "RMSE: 1.1104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4483820563341684"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(model_type='CatBoost', use_important_features=False, classifier_type='SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "855706c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Running Experiment: Model=MLP, Important_Features=False\n",
      "========================================\n",
      "Using all 132 features.\n",
      "Kept features as 1D vectors with dimension 132.\n",
      "Epoch 10/50 | Train: 2.9797 | Val: 3.6832\n",
      "Epoch 20/50 | Train: 2.4301 | Val: 3.3990\n",
      "Epoch 30/50 | Train: 2.1635 | Val: 3.4347\n",
      "Epoch 40/50 | Train: 2.0290 | Val: 3.3087\n",
      "Epoch 50/50 | Train: 1.8008 | Val: 3.1892\n",
      "Early stopping at epoch 50\n",
      "Testing with Best Model...\n",
      "Results for MLP (ImportantFeats=False):\n",
      "MSE: 1.4561\n",
      "RMSE: 1.2067\n",
      "MAE: 0.5676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5676350153069265"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(model_type='MLP', use_important_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41c57a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Running Experiment: Model=MLP, Important_Features=True\n",
      "========================================\n",
      "Using 35 important features.\n",
      "Kept features as 1D vectors with dimension 35.\n",
      "Epoch 10/50 | Train: 3.4125 | Val: 3.7716\n",
      "Epoch 20/50 | Train: 3.1117 | Val: 3.6147\n",
      "Epoch 30/50 | Train: 2.8713 | Val: 3.6025\n",
      "Epoch 40/50 | Train: 2.8316 | Val: 3.4818\n",
      "Epoch 50/50 | Train: 2.6356 | Val: 3.4947\n",
      "Testing with Best Model...\n",
      "Results for MLP (ImportantFeats=True):\n",
      "MSE: 1.4731\n",
      "RMSE: 1.2137\n",
      "MAE: 0.6180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6180294933973991"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(model_type='MLP', use_important_features=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
